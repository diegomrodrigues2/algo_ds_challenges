# ğŸ¯ K-Nearest Neighbors (k-NN) para RegressÃ£o

## ğŸ“– DefiniÃ§Ã£o
**k-NN** Ã© um algoritmo **nÃ£o paramÃ©trico** e **lazy learning** que prediz valores baseado na mÃ©dia dos k vizinhos mais prÃ³ximos no espaÃ§o de caracterÃ­sticas.

## ğŸ”§ MecÃ¢nica do Algoritmo

### ğŸ“ CÃ¡lculo de DistÃ¢ncia

$d(x_1, x_2) = \sqrt{\sum_{i=1}^{n} (x_{1i} - x_{2i})^2}$

### ğŸ¯ PrediÃ§Ã£o

$\hat{y} = \frac{1}{k} \sum_{i \in N_k(x_0)} y_i$

## âš¡ CaracterÃ­sticas Principais

| Aspecto | âœ… Vantagem | âŒ Desvantagem |
|---------|-------------|----------------|
| **Simplicidade** | ğŸ§  FÃ¡cil de entender | ğŸ¯ Limitado para padrÃµes complexos |
| **NÃ£o ParamÃ©trico** | ğŸ”„ Adapta-se a qualquer forma | ğŸ’¾ Usa toda memÃ³ria disponÃ­vel |
| **Lazy Learning** | âš¡ Sem treinamento | ğŸŒ PrediÃ§Ã£o lenta |
| **HiperparÃ¢metro k** | ğŸ›ï¸ Controle de suavidade | ğŸ” DifÃ­cil escolha Ã³tima |

## ğŸ›ï¸ Efeito do HiperparÃ¢metro k

### ğŸ“Š Comportamento
- **k = 1**: PrediÃ§Ã£o = valor do vizinho mais prÃ³ximo
- **k = n**: PrediÃ§Ã£o = mÃ©dia global (muito suave)
- **k Ã³timo**: Balanceia bias e variÃ¢ncia

### ğŸ“ˆ Trade-off Suavidade vs. PrecisÃ£o
```mermaid
graph LR
    A[k pequeno] --> B[Menos suave]
    A --> C[Maior variÃ¢ncia]
    D[k grande] --> E[Mais suave]
    D --> F[Maior bias]
```

## ğŸ” Algoritmo Passo a Passo

1. **ğŸ“ Calcular distÃ¢ncias** entre ponto de consulta e todos os pontos de treino
2. **ğŸ¯ Encontrar k vizinhos** com menores distÃ¢ncias
3. **ğŸ“Š Calcular mÃ©dia** dos valores y dos k vizinhos
4. **ğŸ¯ Retornar prediÃ§Ã£o**

## ğŸš€ Complexidade

| OperaÃ§Ã£o | Tempo | EspaÃ§o |
|----------|-------|--------|
| **Treinamento** | O(1) | O(n) |
| **PrediÃ§Ã£o** | O(n) | O(1) |

## ğŸ¯ Casos de Uso

### âœ… Ideal para:
- ğŸ“Š Dados com padrÃµes locais
- ğŸ” ExploraÃ§Ã£o rÃ¡pida de dados
- ğŸ§ª Baseline simples

### âŒ Evitar quando:
- ğŸ“ˆ Dados de alta dimensionalidade
- âš¡ PrediÃ§Ãµes em tempo real
- ğŸ’¾ MemÃ³ria limitada

## ğŸ”§ ImplementaÃ§Ã£o PrÃ¡tica

### ğŸ“‹ ValidaÃ§Ãµes Essenciais
```python
if k <= 0: raise ValueError("k deve ser positivo")
if k > n_samples: raise ValueError("k muito grande")
```

### ğŸ¯ OtimizaÃ§Ãµes Comuns
- **KD-Trees**: Para dados multidimensionais
- **Ball Trees**: Para mÃ©tricas nÃ£o euclidianas
- **Locality Sensitive Hashing**: Para datasets grandes

## ğŸ“š ReferÃªncias TeÃ³ricas
- **ESL SeÃ§Ã£o 2.3.2**: Fundamentos teÃ³ricos
- **Curse of Dimensionality**: LimitaÃ§Ã£o em alta dimensÃ£o
- **Bias-Variance Trade-off**: Teoria de generalizaÃ§Ã£o 