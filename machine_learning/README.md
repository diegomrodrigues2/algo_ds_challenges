# ğŸ¤– Machine Learning

Este diretÃ³rio contÃ©m implementaÃ§Ãµes de algoritmos de Machine Learning em Python, focando em implementaÃ§Ãµes **from-scratch** para aprendizado e compreensÃ£o profunda dos conceitos.

## ğŸ“š Estrutura

```
machine_learning/
â”œâ”€â”€ supervised_learning/     # Aprendizado Supervisionado
â”‚   â”œâ”€â”€ knn_regression.py   # K-Nearest Neighbors para RegressÃ£o
â”‚   â””â”€â”€ test_knn_regression.py
â”œâ”€â”€ unsupervised_learning/   # Aprendizado NÃ£o Supervisionado
â”œâ”€â”€ optimization/           # Algoritmos de OtimizaÃ§Ã£o
â”œâ”€â”€ feature_engineering/    # Engenharia de Features
â””â”€â”€ evaluation/            # MÃ©tricas de AvaliaÃ§Ã£o
```

## ğŸ¯ Desafios DisponÃ­veis

### Supervised Learning

#### K-Nearest Neighbors (k-NN) para RegressÃ£o
- **Arquivo**: `supervised_learning/knn_regression.py`
- **Testes**: `supervised_learning/test_knn_regression.py`
- **Conceito**: Algoritmo nÃ£o paramÃ©trico que prediz valores baseado na mÃ©dia dos k vizinhos mais prÃ³ximos
- **FunÃ§Ãµes a implementar**:
  - `euclidean_distance()`: CÃ¡lculo de distÃ¢ncia euclidiana
  - `find_k_nearest_neighbors()`: Busca pelos k vizinhos mais prÃ³ximos
  - `knn_regress()`: PrediÃ§Ã£o principal para um ponto
  - `knn_regress_batch()`: PrediÃ§Ã£o em lote para mÃºltiplos pontos
  - `KNNRegressor`: Classe com interface similar ao scikit-learn

#### CaracterÃ­sticas do Desafio:
- **DependÃªncias**: Apenas NumPy
- **Complexidade**: O(n) para prediÃ§Ã£o, O(1) para treinamento
- **Casos de teste**: Inclui dados sintÃ©ticos, validaÃ§Ãµes de entrada, e testes de comportamento esperado
- **ReferÃªncia**: The Elements of Statistical Learning, SeÃ§Ã£o 2.3.2

## ğŸ§ª ExecuÃ§Ã£o dos Testes

```bash
# Executar todos os testes de Machine Learning
pytest machine_learning/

# Executar testes especÃ­ficos do k-NN
pytest machine_learning/supervised_learning/test_knn_regression.py

# Executar com verbose
pytest machine_learning/ -v
```

## ğŸ“‹ PrÃ³ximos Desafios Planejados

### Supervised Learning
- **RegressÃ£o Linear**: ImplementaÃ§Ã£o do gradiente descendente
- **Logistic Regression**: ClassificaÃ§Ã£o binÃ¡ria
- **Decision Trees**: Ãrvores de decisÃ£o para classificaÃ§Ã£o e regressÃ£o
- **Random Forest**: Ensemble de Ã¡rvores de decisÃ£o
- **Support Vector Machines**: SVM para classificaÃ§Ã£o e regressÃ£o

### Unsupervised Learning
- **K-Means Clustering**: Agrupamento baseado em centroides
- **DBSCAN**: Clustering baseado em densidade
- **Principal Component Analysis**: ReduÃ§Ã£o de dimensionalidade

### Optimization
- **Gradient Descent**: OtimizaÃ§Ã£o de primeira ordem
- **Stochastic Gradient Descent**: Variante estocÃ¡stica
- **Adam Optimizer**: Otimizador adaptativo

## ğŸ¯ Objetivos de Aprendizado

1. **CompreensÃ£o TeÃ³rica**: Entender os fundamentos matemÃ¡ticos de cada algoritmo
2. **ImplementaÃ§Ã£o PrÃ¡tica**: Desenvolver algoritmos from-scratch em Python
3. **ValidaÃ§Ã£o**: Criar testes robustos que validem a correÃ§Ã£o das implementaÃ§Ãµes
4. **OtimizaÃ§Ã£o**: Considerar aspectos de performance e eficiÃªncia
5. **ComparaÃ§Ã£o**: Entender trade-offs entre diferentes algoritmos

## ğŸ“š ReferÃªncias

- **The Elements of Statistical Learning**: Hastie, Tibshirani, Friedman
- **Pattern Recognition and Machine Learning**: Bishop
- **Machine Learning**: Mitchell
- **Scikit-learn Documentation**: Para comparaÃ§Ã£o com implementaÃ§Ãµes de produÃ§Ã£o

## ğŸ”§ ConvenÃ§Ãµes

- **DependÃªncias**: Usar apenas NumPy alÃ©m do Python padrÃ£o
- **DocumentaÃ§Ã£o**: Incluir docstrings explicativas
- **Testes**: Cobertura completa com casos tÃ­picos e edge cases
- **Performance**: Considerar complexidade temporal e espacial
- **Interface**: Seguir padrÃµes similares ao scikit-learn quando apropriado 